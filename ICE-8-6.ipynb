{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5abc249-ec0a-4032-b810-d447d9bc7235",
   "metadata": {},
   "source": [
    "# This ICE - 8 is being invoked by running the following commands in the terminal:\n",
    "\n",
    " \n",
    "(base) robertfajardo@UroboricForms ~ % source ~/.bash_profile  \\\n",
    "(base) robertfajardo@UroboricForms ~ % pyspark  \n",
    "\n",
    "To be more transparent, the bash_profile contains these two lines of code that activate Jupyter Lab - SparkSession\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter \\\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='lab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2618fe9-eee5-4ba2-8199-5805a5d2dfc8",
   "metadata": {},
   "source": [
    "![SNOWFALL](terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6346-39b7-4487-8d32-1fc2fdcfd4fe",
   "metadata": {},
   "source": [
    "![Check](bashfile.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918c9951-02c5-4e36-9d1f-c5ee46cce914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://uroboric-forms:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113370d60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell confirms the version and pyspark activation\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5f83b4-4802-454f-b1ba-9d73aaada28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This activates(initializes) the spark packages and SparkSession\n",
    "\n",
    "'''Refer to this page --->. https://spark.apache.org/docs/latest/sql-data-sources-text.html'''\n",
    "#file = spark.sparkContext.textFile(\"/Users/robertfajardo/Downloads/ml-100k/u.user\")\n",
    "\n",
    "sc= spark.sparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177ae2e-7222-4b1e-a994-31894aca6e24",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "## Create spark RDD from external dataset(word_list.txt)). Execute transformation and actions by scala. \n",
    "\n",
    "Here, the data is captures from my folder and place into the spark context in RDD using the textFile method. \"This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines.\" (ApaceSpark, 2022)\n",
    "The RDD file name \"file\" can receive dataset operations such as counting, sizing, and map/reduce operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2827c95d-a74a-49e1-89ac-2a43e8dff504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = spark.sparkContext.textFile(\"/Users/robertfajardo/Library/CloudStorage/OneDrive-UNTSystem/UNT Courses/CSCE 5300 Big Data/Week8/word_list-1.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc8edf-fe3e-4c4a-a51b-0e585720cacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a920821-2757-43f0-8db9-11d1e74116ea",
   "metadata": {},
   "source": [
    "## Change all words to uppercase and show the first two lines.\n",
    "\n",
    "Changing all words to upper case allows use to count words regardless of case differentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3856edca-78f5-474b-a555-c58ff6e014bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile_up = file.map(lambda line: line.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e526ea24-560e-4420-8ed6-ed166dc76c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['THE PROJECT GUTENBERG ETEXT OF MOBY WORD II BY GRADY WARD',\n",
       " 'COPYRIGHT LAWS ARE CHANGING ALL OVER THE WORLD, BE SURE TO CHECK']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile_up.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170d75d-3042-424f-aacd-88ac488eec65",
   "metadata": {},
   "source": [
    "## Count the number of lines.\n",
    "\n",
    "This transformation delimits words by line, thus allowing us to count each line as a row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a864fcb2-aec9-4033-9e5f-7d5f85f71e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721c231-dff3-4676-9c5c-c974a3d3057e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count the number of word “PROJECT”. \n",
    "\n",
    "\"flatMap\" transformation applies the function to all elements of the RDD and flattens the results. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b21823-ab3b-453a-8e91-a504e0d13dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile_up.flatMap(lambda line: line.split(\" \"))\\\n",
    "    .filter(lambda word: word.count(\"PROJECT\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330269f-1b77-4498-b986-9a9a98f139f9",
   "metadata": {},
   "source": [
    "## Count the words in the dataset\n",
    "\n",
    "Now, I can count the words with the delimiter space split, mapping the words and assigning a count of 1 per occurance. Then, collecting the results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2edb38-3889-4f07-9824-471f6331d345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wordcount = myfile_up.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b:a+b).collect()\n",
    "    \n",
    "    \n",
    "    #lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e69843-81f5-4e42-9820-6f3033117790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first 20 wordcount results! \n",
      "\n",
      "[('ETEXT', 21), ('OF', 72), ('MOBY', 5), ('WORD', 6), ('II', 5), ('GRADY', 2), ('COPYRIGHT', 8), ('LAWS', 2), ('ARE', 15), ('OVER', 4), ('BE', 15), ('TO', 55), ('CHECK', 1), ('FOR', 21), ('YOUR', 12), ('COUNTRY', 1), ('FILES!!!', 1), ('PLEASE', 5), ('THIS', 42)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Here are the first 20 wordcount results! \\n\\n\"+ str(wordcount[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582f3f2-4158-4a39-ae3d-b1bb9c9400ef",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca4fa4-76a3-41b0-933b-6f5f17f2bd4b",
   "metadata": {},
   "source": [
    "Create spark RDD from external dataset(shakespeare.txt). Execute transformation and actions.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd2bd7-eb51-4964-a78a-6158ec72bb95",
   "metadata": {},
   "source": [
    "## Change all words to lowercase and show the first 5 lines. \n",
    "\n",
    "I'm using the textFile method and giving the output for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3470ed22-c142-4674-b321-f351c4d6167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first 5 lines. Not including the space between lines \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of The Complete Works of William Shakespeare, by ',\n",
       " 'William Shakespeare',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "shakespeare = spark.sparkContext.textFile(\"/Users/robertfajardo/Library/CloudStorage/OneDrive-UNTSystem/UNT Courses/CSCE 5300 Big Data/Week8/shakespeare-1.txt\")\n",
    "\n",
    "print(\"Here are the first 5 lines. Not including the space between lines \\n\\n\")\n",
    "shakespeare.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ef9c4d9-fbab-42e3-9bdb-dc5bbe404833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting words to lowercase\n",
    "myshake_down = shakespeare.map(lambda line: line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e376a42-22ed-4b3e-8cca-181b6da3887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words are now converted to lowercase. Here are the first 5 lines, excluding the space between the lines. \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of the complete works of william shakespeare, by ',\n",
       " 'william shakespeare',\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  you may copy it, give it away or',\n",
       " 're-use it under the terms of the project gutenberg license included']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"All words are now converted to lowercase. Here are the first 5 lines, excluding the space between the lines. \\n\\n\")\n",
    "\n",
    "myshake_down.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29735847-de9d-452f-82f7-78739fe918e0",
   "metadata": {},
   "source": [
    "## Count the total number of words. \n",
    "\n",
    "It's imperative to clean the document of punctuation which could count as a word with the considerable spacing and punctuation in the dataset. \n",
    "In example, \\\n",
    "(\"words.\", 1)<-with period and (\"words\", 1)<- without period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1534abf-92fa-4568-9263-f6128351a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-words, such as punctuation\n",
    "def clean(x):\n",
    "  punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "  words=x.lower()\n",
    "  for ch in punc:\n",
    "    words = words.replace(ch, '')\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "989e5ef5-c3d2-4c41-b636-1bb6a4ca2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myshake_down = myshake_down.map(clean) #remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa99359f-6b17-49cb-930b-088e5e38556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that the last line after the word periods or commas.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of the complete works of william shakespeare by ',\n",
       " 'william shakespeare',\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever  you may copy it give it away or',\n",
       " 'reuse it under the terms of the project gutenberg license included']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Note that the last line after the word periods or commas.\\n\\n\")\n",
    "myshake_down.take(6)  #punctuation is removed. Viewing first 3 lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db5cbdaa-e185-4ecd-abe0-7dbd620df571",
   "metadata": {},
   "outputs": [],
   "source": [
    "myshake_down1= myshake_down.flatMap(lambda satir: satir.split(\" \")) #split and delimit by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f2acf2d-f24e-47e7-819d-b717f9512889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'project', 'gutenberg', 'ebook', 'of']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myshake_down1.take(5)  #all words are split by the space and cleaned of puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "632f39ea-cff6-4f87-91c7-ee595c469bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shakespeare dataset has a total of 1410759 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and has a total of 28494 distinct words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"The shakespeare dataset has a total of \"+ str(myshake_down1.count()) + \" words.\") \n",
    "#punctuation is not counted\n",
    "\n",
    "print(\"and has a total of \"+ str(myshake_down1.distinct().count()) + \" distinct words.\") \n",
    "#punctuation is not counted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203521a4-eb4d-4124-abd2-70c76cbc502f",
   "metadata": {},
   "source": [
    "<h4> Answer: The shakespeare dataset has a total of 1,410,759 words and has a total of 28,494 distinct words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970dcd0-ddda-40e1-a997-85a268f1487e",
   "metadata": {},
   "source": [
    "## Count the number of word “is”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9adbb93f-17e5-4656-9c7b-4845507d60d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36803"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "myshake_down1.flatMap(lambda line: line.split(\" \"))\\\n",
    "    .filter(lambda word: word.count(\"is\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab801415-af90-49de-94e5-0e4513eadb38",
   "metadata": {},
   "source": [
    "<h4>  Answer: The word \"is\" appears 36,803 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8298d0-7eb8-4374-9954-872be0584c04",
   "metadata": {},
   "source": [
    "##  Count the number of unique words in the dataset. \n",
    "\n",
    "The words have already been pertitioned from lines and only need to assign the count per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58e8bd55-5f86-4ce3-bd64-ed95d1f3f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shake_wordcount = myshake_down1.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32693e60-3a80-4dcd-afc1-ecc91723e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('project', 329), ('gutenberg', 257)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake_wordcount.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae1fa9ac-1fcb-4a74-a303-3407ab154b30",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|  project|  329|\n",
      "|gutenberg|  257|\n",
      "|    ebook|   16|\n",
      "+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordcount_columns = [\"word\",\"count\"]\n",
    "count = spark.createDataFrame(shake_wordcount, schema = wordcount_columns)\n",
    "count.printSchema()\n",
    "count.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "daf1c978-d039-405e-809a-116061ddb99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28494"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3855bcb-a019-40d7-bf47-a82ae85556ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 28494 distinct words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of \"+ str(myshake_down1.distinct().count()) + \" distinct words.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99533b09-48e4-40cd-a9f3-80f2f8d01484",
   "metadata": {},
   "source": [
    "<h4> Answer: There is a total of 28,494 unique(distinct) words in the dataset. </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c574702-c3ea-4d45-bcb0-47ff00c61eb2",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e59f7e-db95-4716-88bd-b22427a18725",
   "metadata": {},
   "source": [
    "## Create Spark dataframe from hotel_booking data and execute some query. \n",
    "\n",
    "Previously, we read text files and now we have a .csv file with an option to inferschema and header. \n",
    "In this format, I can use SparkSQL commands to gather statistics and general SQL queries as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00429a14-1e89-42f0-88c4-95b5ca39d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 22:53:16 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.dtabricks.spark.csv\").option(\"mode\",\"DROPFORMALIZED\").option(\"header\", True)\\\n",
    "    .option(\"inferschema\", True).\\\n",
    "    csv(\"/Users/robertfajardo/Library/CloudStorage/OneDrive-UNTSystem/UNT Courses/CSCE 5300 Big Data/Week8/hotel_bookings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1a2a8b8-f654-49db-9d2b-30e3792ef177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hotel: string (nullable = true)\n",
      " |-- is_canceled: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- arrival_date_year: integer (nullable = true)\n",
      " |-- arrival_date_month: string (nullable = true)\n",
      " |-- arrival_date_week_number: integer (nullable = true)\n",
      " |-- arrival_date_day_of_month: integer (nullable = true)\n",
      " |-- stays_in_weekend_nights: integer (nullable = true)\n",
      " |-- stays_in_week_nights: integer (nullable = true)\n",
      " |-- adults: integer (nullable = true)\n",
      " |-- children: string (nullable = true)\n",
      " |-- babies: integer (nullable = true)\n",
      " |-- meal: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- market_segment: string (nullable = true)\n",
      " |-- distribution_channel: string (nullable = true)\n",
      " |-- is_repeated_guest: integer (nullable = true)\n",
      " |-- previous_cancellations: integer (nullable = true)\n",
      " |-- previous_bookings_not_canceled: integer (nullable = true)\n",
      " |-- reserved_room_type: string (nullable = true)\n",
      " |-- assigned_room_type: string (nullable = true)\n",
      " |-- booking_changes: integer (nullable = true)\n",
      " |-- deposit_type: string (nullable = true)\n",
      " |-- agent: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- days_in_waiting_list: integer (nullable = true)\n",
      " |-- customer_type: string (nullable = true)\n",
      " |-- adr: double (nullable = true)\n",
      " |-- required_car_parking_spaces: integer (nullable = true)\n",
      " |-- total_of_special_requests: integer (nullable = true)\n",
      " |-- reservation_status: string (nullable = true)\n",
      " |-- reservation_status_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd480a-a87e-40c9-8179-fc914b2b0942",
   "metadata": {},
   "source": [
    "## Get statistics on adults.\n",
    "\n",
    " \n",
    "Show some statistical values(mean, max value) of adults column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dad7067-d61e-45d5-be64-a31ebb6a1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 21:58:43 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "22/04/11 21:58:43 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "[Stage 40:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            adults|\n",
      "+-------+------------------+\n",
      "|  count|            119390|\n",
      "|   mean|1.8564033838679956|\n",
      "| stddev|0.5792609988327547|\n",
      "|    min|                 0|\n",
      "|    max|                55|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe(\"adults\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f38a8-30d4-4c17-8d1d-859c381254ae",
   "metadata": {},
   "source": [
    "## Count total number of canceled by hotel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ceb6442-a45e-4e4e-be53-d0f9b3229bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 21:58:45 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "22/04/11 21:58:45 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "[Stage 43:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|       hotel|sum(is_canceled)|\n",
      "+------------+----------------+\n",
      "|Resort Hotel|           11122|\n",
      "|  City Hotel|           33102|\n",
      "+------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby(\"hotel\").sum(\"is_canceled\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb784d7-8f2d-4bbc-8310-026eccfc4cc6",
   "metadata": {},
   "source": [
    "## Register the DataFrame as a global temporary view.\n",
    "\n",
    "While in global temp view, we can now gather similar statistics using general knowledge of SQL queries invoked inside spark.sql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05f20282-a1cf-47e7-a0bc-5c886fdbebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView(\"hotel\")\n",
    "\n",
    "#As of version 2.2.0, in order to re-run the notebook, running only the df.createGlobalTempView(\"hoteldf\") multiple times\n",
    "    #generates an error. Therefore we need to utilize createOrReplaceGlobalTempView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e7eb3-6033-4230-8822-2256fa7214fb",
   "metadata": {},
   "source": [
    "## Use query to count number of records is reservation_status=”canceled”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "350a2bfd-8ec8-49e3-9dc7-036d9d3f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled = spark.sql('''\n",
    "Select count(reservation_status) as cancel from global_temp.hotel where reservation_status='Canceled' \n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08e47ed8-7ceb-469a-86a3-187bcc776eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 21:59:18 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "22/04/11 21:59:18 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "[Stage 46:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|cancel|\n",
      "+------+\n",
      "| 43017|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cancelled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31a718-451f-46dd-9521-5d0e76d3cd0b",
   "metadata": {},
   "source": [
    "## Use query to count the number of agents.  Group by hotel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13346036-6da6-4ddc-846f-d393953e7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = spark.sql('''\n",
    "Select sum(agent) as agents_count, hotel from global_temp.hotel group by hotel\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "719c3632-a18b-40e9-bc2c-2565ecdcd57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 21:59:25 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "22/04/11 21:59:25 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|agents_count|       hotel|\n",
      "+------------+------------+\n",
      "|   6929877.0|Resort Hotel|\n",
      "|   2003876.0|  City Hotel|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829debd-4e2f-47d0-aad7-70fa9f51bc86",
   "metadata": {},
   "source": [
    "## Use query to count the number of babies when babies are greater than 0 by year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e792e29-36ee-427f-b98e-258fff419f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_number = spark.sql('''\n",
    "\n",
    "Select arrival_date_year, count(babies) as baby_count \n",
    "\n",
    "from global_temp.hotel where babies>0 group by arrival_date_year ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c4a18cc-6881-4238-b300-74f3e0dd4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 21:59:37 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "22/04/11 21:59:37 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "[Stage 52:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|arrival_date_year|baby_count|\n",
      "+-----------------+----------+\n",
      "|             2015|       213|\n",
      "|             2016|       446|\n",
      "|             2017|       258|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "baby_number.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c1fa0-67b3-4c75-b308-a504eb83da64",
   "metadata": {},
   "source": [
    "## Use query to sort the number of canceled by country in decreasing order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34d3c25f-a39d-4fb7-bfd8-66bbf3f18041",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_number = spark.sql('''\n",
    "\n",
    "select country, sum(is_canceled) as all_canceled \n",
    "    from global_temp.hotel\n",
    "        group by country \n",
    "            order by all_canceled desc  \n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f60d4ddb-7721-4f91-a8bc-f0aba0f31fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/11 21:59:51 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "22/04/11 21:59:51 WARN ParseMode: DROPFORMALIZED is not a valid parse mode. Using PERMISSIVE.\n",
      "[Stage 55:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|country|all_canceled|\n",
      "+-------+------------+\n",
      "|    PRT|       27519|\n",
      "|    GBR|        2453|\n",
      "|    ESP|        2177|\n",
      "|    FRA|        1934|\n",
      "|    ITA|        1333|\n",
      "|    DEU|        1218|\n",
      "|    IRL|         832|\n",
      "|    BRA|         830|\n",
      "|    USA|         501|\n",
      "|    BEL|         474|\n",
      "|    CHN|         462|\n",
      "|    CHE|         428|\n",
      "|    NLD|         387|\n",
      "|     CN|         254|\n",
      "|    RUS|         239|\n",
      "|    AUT|         230|\n",
      "|    SWE|         227|\n",
      "|    POL|         215|\n",
      "|    AGO|         205|\n",
      "|    NOR|         181|\n",
      "+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cancel_number.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf15b0-0fb4-4b8d-984e-95dc04f48bb4",
   "metadata": {},
   "source": [
    "## References: \n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations \\\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html \\\n",
    "https://sparkbyexamples.com/pyspark-rdd \\\n",
    "https://sparkbyexamples.com/pyspark/different-ways-to-create-dataframe-in-pyspark/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b2ba4-0a6e-4b75-ad8d-98d4d0e89eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
